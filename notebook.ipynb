{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">\n",
    "\n",
    "## Section 1. Introduction to the Problem/Task\n",
    "\n",
    "**The Problem**\n",
    "Navigating extensive legal and technical documents, such as the Philippine DOLE Occupational Safety and Health Standards (OSHS), presents a significant \"information bottleneck.\" Finding specific compliance metrics, hazard guidelines, or equipment specifications via manual search is inefficient and prone to human error. Furthermore, while standard Large Language Models (LLMs) are highly capable conversational agents, they cannot be trusted with critical safety queries out-of-the-box because they are prone to \"hallucinating\" technical facts and lack innate knowledge of localized policy documents.\n",
    "\n",
    "**Purpose and Domain Use Case**\n",
    "The purpose of this project is to develop an LLM-powered chatbot tailored specifically to the domain of workplace safety policies and manuals. The intended use case is to serve as an interactive safety assistant for safety officers, employers, and workers. Users can query the system in natural language (e.g., \"What are the required dimensions for a machine guard?\") and the chatbot will instantly retrieve and synthesize the exact procedural guidelines and compliance protocols from the official DOLE OSHS text.\n",
    "\n",
    "**Real-World Significance**\n",
    "Building a retrieval-grounded conversational system (utilizing a Retrieval-Augmented Generation or RAG pipeline) is critical for this application. By anchoring the LLM's responses exclusively to retrieved chunks of the official OSHS document, we eliminate hallucinations and guarantee that the information provided is factual, reliable, and citeable. In a real-world setting, this system accelerates regulatory compliance, democratizes access to dense safety protocols, and ultimately helps mitigate workplace hazards by ensuring accurate safety knowledge is instantly accessible.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2. Dataset Description\n",
    "\n",
    "**Knowledge Source and Collection**\n",
    "The primary knowledge source for this chatbot is the **Occupational Safety and Health Standards (OSHS) As Amended** handbook, issued by the Department of Labor and Employment (DOLE) of the Philippines. The document was acquired as a digital PDF (closed-corpus) and serves as the definitive legal and regulatory baseline for occupational safety in the country. \n",
    "\n",
    "**Dataset Structure**\n",
    "* **Format:** Single PDF document (`Osh-Handbook.pdf`)\n",
    "* **Domain:** Legal, Regulatory, and Occupational Health & Safety\n",
    "* **Contents:** The document is highly structured, consisting of hierarchical legal frameworks (Rules, Sections, Sub-sections) alongside dense technical matrices (e.g., Threshold Limit Values for airborne contaminants, medical supply requirements).\n",
    "\n",
    "**Preprocessing and Data Pipeline**\n",
    "To ensure the LLM accurately retrieves and contextualizes the legal statutes without hallucination, standard naive chunking was discarded in favor of a **Structure-Aware Processing Pipeline**:\n",
    "\n",
    "1.  **Document Cleaning:** * **Artifact Removal:** Page numbers, headers, and extraneous source tags (e.g., `--- PAGE X ---`) were stripped using Regular Expressions to reduce embedding noise.\n",
    "    * **Hyphenation Merging:** Words split across line breaks by hyphens (e.g., \"equip-ment\") were systematically rejoined to maintain semantic integrity during vector search.\n",
    "2.  **Handling Tables:** * Complex tables embedded within the PDF are extracted independently using `pdfplumber`. These tables are converted into Markdown format before embedding to preserve their row-column relationships, ensuring that specific numerical limits and chemical properties remain explicitly linked to their respective entities.\n",
    "3.  **Structure-Aware Chunking & Metadata Tagging:** * The text is strictly partitioned using **Rule Numbers** (e.g., \"Rule 1040\") as the primary delimiters. \n",
    "    * **Context Injection:** To prevent orphaned text chunks from losing their legal context, the specific Rule Number and Title are prepended as metadata to every sub-chunk generated from that section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.1 Dataset Cleaning\n",
    "\n",
    "#### Environment Setup and Imports\n",
    "Run this first to install the required libraries and import the modules. tabulate is required for pandas to convert tables to Markdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pdfplumber'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Import modules\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mre\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpdfplumber\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_text_splitters\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RecursiveCharacterTextSplitter\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pdfplumber'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pdfplumber\n",
      "  Downloading pdfplumber-0.11.9-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting langchain\n",
      "  Downloading langchain-1.2.10-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting langchain-text-splitters\n",
      "  Downloading langchain_text_splitters-1.1.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting pandas\n",
      "  Downloading pandas-3.0.1-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
      "Collecting tabulate\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Collecting pdfminer.six==20251230 (from pdfplumber)\n",
      "  Downloading pdfminer_six-20251230-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting Pillow>=9.1 (from pdfplumber)\n",
      "  Downloading pillow-12.1.1-cp312-cp312-win_amd64.whl.metadata (9.0 kB)\n",
      "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
      "  Downloading pypdfium2-5.5.0-py3-none-win_amd64.whl.metadata (68 kB)\n",
      "Collecting charset-normalizer>=2.0.0 (from pdfminer.six==20251230->pdfplumber)\n",
      "  Downloading charset_normalizer-3.4.4-cp312-cp312-win_amd64.whl.metadata (38 kB)\n",
      "Collecting cryptography>=36.0.0 (from pdfminer.six==20251230->pdfplumber)\n",
      "  Downloading cryptography-46.0.5-cp311-abi3-win_amd64.whl.metadata (5.7 kB)\n",
      "Collecting langchain-core<2.0.0,>=1.2.10 (from langchain)\n",
      "  Downloading langchain_core-1.2.14-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting langgraph<1.1.0,>=1.0.8 (from langchain)\n",
      "  Downloading langgraph-1.0.9-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting pydantic<3.0.0,>=2.7.4 (from langchain)\n",
      "  Downloading pydantic-2.12.5-py3-none-any.whl.metadata (90 kB)\n",
      "Collecting numpy>=1.26.0 (from pandas)\n",
      "  Downloading numpy-2.4.2-cp312-cp312-win_amd64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\kurt\\onedrive\\documents\\projects\\osh-standards-chatbot\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting tzdata (from pandas)\n",
      "  Downloading tzdata-2025.3-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting jsonpatch<2.0.0,>=1.33.0 (from langchain-core<2.0.0,>=1.2.10->langchain)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting langsmith<1.0.0,>=0.3.45 (from langchain-core<2.0.0,>=1.2.10->langchain)\n",
      "  Downloading langsmith-0.7.6-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: packaging>=23.2.0 in c:\\users\\kurt\\onedrive\\documents\\projects\\osh-standards-chatbot\\.venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.10->langchain) (26.0)\n",
      "Collecting pyyaml<7.0.0,>=5.3.0 (from langchain-core<2.0.0,>=1.2.10->langchain)\n",
      "  Downloading pyyaml-6.0.3-cp312-cp312-win_amd64.whl.metadata (2.4 kB)\n",
      "Collecting tenacity!=8.4.0,<10.0.0,>=8.1.0 (from langchain-core<2.0.0,>=1.2.10->langchain)\n",
      "  Downloading tenacity-9.1.4-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting typing-extensions<5.0.0,>=4.7.0 (from langchain-core<2.0.0,>=1.2.10->langchain)\n",
      "  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting uuid-utils<1.0,>=0.12.0 (from langchain-core<2.0.0,>=1.2.10->langchain)\n",
      "  Downloading uuid_utils-0.14.1-cp39-abi3-win_amd64.whl.metadata (4.9 kB)\n",
      "Collecting langgraph-checkpoint<5.0.0,>=2.1.0 (from langgraph<1.1.0,>=1.0.8->langchain)\n",
      "  Downloading langgraph_checkpoint-4.0.0-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting langgraph-prebuilt<1.1.0,>=1.0.8 (from langgraph<1.1.0,>=1.0.8->langchain)\n",
      "  Downloading langgraph_prebuilt-1.0.8-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting langgraph-sdk<0.4.0,>=0.3.0 (from langgraph<1.1.0,>=1.0.8->langchain)\n",
      "  Downloading langgraph_sdk-0.3.8-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting xxhash>=3.5.0 (from langgraph<1.1.0,>=1.0.8->langchain)\n",
      "  Downloading xxhash-3.6.0-cp312-cp312-win_amd64.whl.metadata (13 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.41.5 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Downloading pydantic_core-2.41.5-cp312-cp312-win_amd64.whl.metadata (7.4 kB)\n",
      "Collecting typing-inspection>=0.4.2 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Downloading typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\kurt\\onedrive\\documents\\projects\\osh-standards-chatbot\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Collecting cffi>=2.0.0 (from cryptography>=36.0.0->pdfminer.six==20251230->pdfplumber)\n",
      "  Downloading cffi-2.0.0-cp312-cp312-win_amd64.whl.metadata (2.6 kB)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.10->langchain)\n",
      "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting ormsgpack>=1.12.0 (from langgraph-checkpoint<5.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.8->langchain)\n",
      "  Downloading ormsgpack-1.12.2-cp312-cp312-win_amd64.whl.metadata (3.3 kB)\n",
      "Collecting httpx>=0.25.2 (from langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.8->langchain)\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting orjson>=3.11.5 (from langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.8->langchain)\n",
      "  Downloading orjson-3.11.7-cp312-cp312-win_amd64.whl.metadata (43 kB)\n",
      "Collecting requests-toolbelt>=1.0.0 (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.10->langchain)\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting requests>=2.0.0 (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.10->langchain)\n",
      "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting zstandard>=0.23.0 (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.10->langchain)\n",
      "  Downloading zstandard-0.25.0-cp312-cp312-win_amd64.whl.metadata (3.3 kB)\n",
      "Collecting pycparser (from cffi>=2.0.0->cryptography>=36.0.0->pdfminer.six==20251230->pdfplumber)\n",
      "  Downloading pycparser-3.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting anyio (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.8->langchain)\n",
      "  Downloading anyio-4.12.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting certifi (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.8->langchain)\n",
      "  Downloading certifi-2026.1.4-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting httpcore==1.* (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.8->langchain)\n",
      "  Downloading httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting idna (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.8->langchain)\n",
      "  Downloading idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.8->langchain)\n",
      "  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.10->langchain)\n",
      "  Downloading urllib3-2.6.3-py3-none-any.whl.metadata (6.9 kB)\n",
      "Downloading pdfplumber-0.11.9-py3-none-any.whl (60 kB)\n",
      "Downloading pdfminer_six-20251230-py3-none-any.whl (6.6 MB)\n",
      "   ---------------------------------------- 0.0/6.6 MB ? eta -:--:--\n",
      "   ----------- ---------------------------- 1.8/6.6 MB 10.1 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 3.1/6.6 MB 7.7 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 4.2/6.6 MB 7.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 5.8/6.6 MB 7.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.6/6.6 MB 7.0 MB/s eta 0:00:00\n",
      "Downloading langchain-1.2.10-py3-none-any.whl (111 kB)\n",
      "Downloading langchain_text_splitters-1.1.1-py3-none-any.whl (35 kB)\n",
      "Downloading pandas-3.0.1-cp312-cp312-win_amd64.whl (9.7 MB)\n",
      "   ---------------------------------------- 0.0/9.7 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 1.6/9.7 MB 7.6 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 3.7/9.7 MB 8.7 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 5.0/9.7 MB 7.9 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 6.8/9.7 MB 8.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 9.2/9.7 MB 8.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.7/9.7 MB 8.5 MB/s eta 0:00:00\n",
      "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Downloading langchain_core-1.2.14-py3-none-any.whl (501 kB)\n",
      "Downloading langgraph-1.0.9-py3-none-any.whl (158 kB)\n",
      "Downloading numpy-2.4.2-cp312-cp312-win_amd64.whl (12.3 MB)\n",
      "   ---------------------------------------- 0.0/12.3 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 2.9/12.3 MB 15.2 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 5.8/12.3 MB 13.6 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 8.7/12.3 MB 14.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.8/12.3 MB 14.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.3/12.3 MB 14.0 MB/s eta 0:00:00\n",
      "Downloading pillow-12.1.1-cp312-cp312-win_amd64.whl (7.0 MB)\n",
      "   ---------------------------------------- 0.0/7.0 MB ? eta -:--:--\n",
      "   -------------- ------------------------- 2.6/7.0 MB 12.6 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 5.5/7.0 MB 13.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.0/7.0 MB 12.8 MB/s eta 0:00:00\n",
      "Downloading pydantic-2.12.5-py3-none-any.whl (463 kB)\n",
      "Downloading pydantic_core-2.41.5-cp312-cp312-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.0/2.0 MB 14.1 MB/s eta 0:00:00\n",
      "Downloading pypdfium2-5.5.0-py3-none-win_amd64.whl (3.1 MB)\n",
      "   ---------------------------------------- 0.0/3.1 MB ? eta -:--:--\n",
      "   ------------------------------ --------- 2.4/3.1 MB 12.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.1/3.1 MB 11.4 MB/s eta 0:00:00\n",
      "Downloading tzdata-2025.3-py2.py3-none-any.whl (348 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading charset_normalizer-3.4.4-cp312-cp312-win_amd64.whl (107 kB)\n",
      "Downloading cryptography-46.0.5-cp311-abi3-win_amd64.whl (3.5 MB)\n",
      "   ---------------------------------------- 0.0/3.5 MB ? eta -:--:--\n",
      "   --------------------------- ------------ 2.4/3.5 MB 12.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.5/3.5 MB 12.1 MB/s eta 0:00:00\n",
      "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading langgraph_checkpoint-4.0.0-py3-none-any.whl (46 kB)\n",
      "Downloading langgraph_prebuilt-1.0.8-py3-none-any.whl (35 kB)\n",
      "Downloading langgraph_sdk-0.3.8-py3-none-any.whl (90 kB)\n",
      "Downloading langsmith-0.7.6-py3-none-any.whl (325 kB)\n",
      "Downloading pyyaml-6.0.3-cp312-cp312-win_amd64.whl (154 kB)\n",
      "Downloading tenacity-9.1.4-py3-none-any.whl (28 kB)\n",
      "Using cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Downloading typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Downloading uuid_utils-0.14.1-cp39-abi3-win_amd64.whl (187 kB)\n",
      "Downloading xxhash-3.6.0-cp312-cp312-win_amd64.whl (31 kB)\n",
      "Downloading cffi-2.0.0-cp312-cp312-win_amd64.whl (183 kB)\n",
      "Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Downloading httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading orjson-3.11.7-cp312-cp312-win_amd64.whl (125 kB)\n",
      "Downloading ormsgpack-1.12.2-cp312-cp312-win_amd64.whl (117 kB)\n",
      "Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Downloading zstandard-0.25.0-cp312-cp312-win_amd64.whl (506 kB)\n",
      "Downloading certifi-2026.1.4-py3-none-any.whl (152 kB)\n",
      "Downloading idna-3.11-py3-none-any.whl (71 kB)\n",
      "Downloading urllib3-2.6.3-py3-none-any.whl (131 kB)\n",
      "Downloading anyio-4.12.1-py3-none-any.whl (113 kB)\n",
      "Downloading pycparser-3.0-py3-none-any.whl (48 kB)\n",
      "Downloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Installing collected packages: zstandard, xxhash, uuid-utils, urllib3, tzdata, typing-extensions, tenacity, tabulate, pyyaml, pypdfium2, pycparser, Pillow, ormsgpack, orjson, numpy, jsonpointer, idna, h11, charset-normalizer, certifi, annotated-types, typing-inspection, requests, pydantic-core, pandas, jsonpatch, httpcore, cffi, anyio, requests-toolbelt, pydantic, httpx, cryptography, pdfminer.six, langsmith, langgraph-sdk, pdfplumber, langchain-core, langgraph-checkpoint, langchain-text-splitters, langgraph-prebuilt, langgraph, langchain\n",
      "Successfully installed Pillow-12.1.1 annotated-types-0.7.0 anyio-4.12.1 certifi-2026.1.4 cffi-2.0.0 charset-normalizer-3.4.4 cryptography-46.0.5 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 idna-3.11 jsonpatch-1.33 jsonpointer-3.0.0 langchain-1.2.10 langchain-core-1.2.14 langchain-text-splitters-1.1.1 langgraph-1.0.9 langgraph-checkpoint-4.0.0 langgraph-prebuilt-1.0.8 langgraph-sdk-0.3.8 langsmith-0.7.6 numpy-2.4.2 orjson-3.11.7 ormsgpack-1.12.2 pandas-3.0.1 pdfminer.six-20251230 pdfplumber-0.11.9 pycparser-3.0 pydantic-2.12.5 pydantic-core-2.41.5 pypdfium2-5.5.0 pyyaml-6.0.3 requests-2.32.5 requests-toolbelt-1.0.0 tabulate-0.9.0 tenacity-9.1.4 typing-extensions-4.15.0 typing-inspection-0.4.2 tzdata-2025.3 urllib3-2.6.3 uuid-utils-0.14.1 xxhash-3.6.0 zstandard-0.25.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries\n",
    "!pip install pdfplumber langchain langchain-text-splitters pandas tabulate\n",
    "\n",
    "# Import modules\n",
    "import re\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Define the file path (Ensure your PDF is uploaded to the Colab files section)\n",
    "PDF_PATH = \"Osh-Handbook.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Document Cleaning Utility\n",
    "This cell defines the function used to strip out page numbers, source tags, and fix broken words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Removes PDF artifacts and merges hyphenated words.\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove page artifacts like \"--- PAGE 1 ---\"\n",
    "    text = re.sub(r'--- PAGE \\d+ ---', '', text)\n",
    "    \n",
    "    # Merge hyphenated words across newlines (e.g., \"work-\\nplace\" -> \"workplace\")\n",
    "    text = re.sub(r'(\\w+)-\\n(\\w+)', r'\\1\\2', text)\n",
    "    \n",
    "    # Clean up excessive newlines\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
    "    \n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table Extraction\n",
    "This cell handles extracting complex tables and converting them into Markdown so the LLM can understand the rows and columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tables_to_documents(pdf_path):\n",
    "    print(\"Extracting tables...\")\n",
    "    table_documents = []\n",
    "    \n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page_num, page in enumerate(pdf.pages):\n",
    "            tables = page.extract_tables()\n",
    "            for i, table in enumerate(tables):\n",
    "                # Minimum content filter for tables\n",
    "                if not table or len(table) < 2: \n",
    "                    continue \n",
    "                \n",
    "                # Harden headers: Convert None to \"\" and ensure unique column names\n",
    "                raw_headers = [str(col) if col is not None else f\"Col_{j}\" for j, col in enumerate(table[0])]\n",
    "                \n",
    "                # Deduplicate headers if PDF parsing messed up (e.g., two columns named \"Limit\")\n",
    "                headers = pd.Series(raw_headers).mask(pd.Series(raw_headers).duplicated(), \n",
    "                                                      pd.Series(raw_headers) + '_dup').tolist()\n",
    "                \n",
    "                try:\n",
    "                    df = pd.DataFrame(table[1:], columns=headers).dropna(how='all')\n",
    "                    df = df.fillna(\"\") \n",
    "                    md_table = df.to_markdown(index=False)\n",
    "                    \n",
    "                    # Deduplication/Noise filter: Skip tiny or empty tables\n",
    "                    if len(md_table.strip()) < 50:\n",
    "                        continue\n",
    "                        \n",
    "                    # Create structured LangChain Document\n",
    "                    doc = Document(\n",
    "                        page_content=f\"[Table Extracted from Page {page_num + 1}]\\n{md_table}\",\n",
    "                        metadata={\n",
    "                            \"source\": \"Osh-Handbook.pdf\",\n",
    "                            \"page\": page_num + 1,\n",
    "                            \"type\": \"table\",\n",
    "                            \"table_index\": i\n",
    "                        }\n",
    "                    )\n",
    "                    table_documents.append(doc)\n",
    "                except Exception as e:\n",
    "                    print(f\"Skipped broken table on page {page_num + 1}: {e}\")\n",
    "                \n",
    "    print(f\"Successfully extracted {len(table_documents)} table documents.\")\n",
    "    return table_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Extraction & Structure-Aware Chunking\n",
    "This is the core logic. It reads the text, splits it by DOLE Rules, and prepends the Rule Title to every sub-chunk so context is never lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dole_rules_to_documents(pdf_path):\n",
    "    print(\"Extracting and cleaning text...\")\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=150,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    "    )\n",
    "    \n",
    "    text_documents = []\n",
    "    seen_chunks = set()  # Deduplicate by (rule_id, chunk)\n",
    "    \n",
    "    print(\"Chunking rules and assigning metadata...\")\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page_num, page in enumerate(pdf.pages, start=1):\n",
    "            extracted = page.extract_text()\n",
    "            if not extracted:\n",
    "                continue\n",
    "            \n",
    "            cleaned_page_text = clean_text(extracted)\n",
    "            if not cleaned_page_text:\n",
    "                continue\n",
    "            \n",
    "            rule_splits = re.split(r'(?i)\\n(?=Rule\\s\\d{4})', cleaned_page_text)\n",
    "            \n",
    "            for section in rule_splits:\n",
    "                section = section.strip()\n",
    "                \n",
    "                # Minimum content filter\n",
    "                if len(section) < 50:\n",
    "                    continue\n",
    "                    \n",
    "                first_line = section.split('\\n')[0]\n",
    "                rule_match = re.match(r'(?i)Rule\\s(\\d{4})', first_line)\n",
    "                \n",
    "                # Extract specific rule ID for metadata\n",
    "                rule_id = rule_match.group(1) if rule_match else \"General\"\n",
    "                rule_title = first_line.strip() if first_line.strip() else \"General OSHS Provision\"\n",
    "                \n",
    "                sub_chunks = text_splitter.split_text(section)\n",
    "                \n",
    "                for chunk in sub_chunks:\n",
    "                    normalized_chunk = chunk.strip()\n",
    "                    \n",
    "                    # Minimum content filter per chunk\n",
    "                    if len(normalized_chunk.split()) < 10:  # Skip chunks with fewer than 10 words\n",
    "                        continue\n",
    "                    \n",
    "                    # Deduplication check (rule-aware)\n",
    "                    dedup_key = (rule_id, normalized_chunk)\n",
    "                    if dedup_key in seen_chunks:\n",
    "                        continue\n",
    "                    seen_chunks.add(dedup_key)\n",
    "                        \n",
    "                    # Create LangChain Document with rich metadata\n",
    "                    doc = Document(\n",
    "                        page_content=f\"[{rule_title}]\\n{normalized_chunk}\",\n",
    "                        metadata={\n",
    "                            \"source\": \"Osh-Handbook.pdf\",\n",
    "                            \"rule_id\": rule_id,\n",
    "                            \"type\": \"text\",\n",
    "                            \"page\": page_num\n",
    "                        }\n",
    "                    )\n",
    "                    text_documents.append(doc)\n",
    "            \n",
    "    print(f\"Generated {len(text_documents)} structured text documents.\")\n",
    "    return text_documents\n",
    "\n",
    "# Execution\n",
    "tables_docs = extract_tables_to_documents(PDF_PATH)\n",
    "text_docs = process_dole_rules_to_documents(PDF_PATH)\n",
    "all_knowledge_base_docs = text_docs + tables_docs\n",
    "\n",
    "# Preview the rich metadata\n",
    "print(\"\\n--- Document Object Preview ---\")\n",
    "if all_knowledge_base_docs:\n",
    "    preview_index = min(50, len(all_knowledge_base_docs) - 1)\n",
    "    print(f\"Content: {all_knowledge_base_docs[preview_index].page_content[:100]}...\")\n",
    "    print(f\"Metadata: {all_knowledge_base_docs[preview_index].metadata}\")\n",
    "else:\n",
    "    print(\"No documents were generated. Check PDF path and extraction logic.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine & Final Check\n",
    "Run this cell to combine your extracted tables and text chunks into one unified knowledge base list, ready to be embedded and stored in ChromaDB in your next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine text and table documents\n",
    "all_knowledge_base_docs = text_docs + tables_docs\n",
    "\n",
    "print(f\"Total Text Docs: {len(text_docs)}\")\n",
    "print(f\"Total Table Docs: {len(tables_docs)}\")\n",
    "print(f\"Total Combined Docs ready for Vector DB: {len(all_knowledge_base_docs)}\")\n",
    "\n",
    "# This list 'all_knowledge_base_docs' is what you will pass to your embedding model!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
