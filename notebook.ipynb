{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">\n",
    "\n",
    "## Section 1. Introduction to the Problem/Task\n",
    "\n",
    "**The Problem**\n",
    "Navigating extensive legal and technical documents, such as the Philippine DOLE Occupational Safety and Health Standards (OSHS), presents a significant \"information bottleneck.\" Finding specific compliance metrics, hazard guidelines, or equipment specifications via manual search is inefficient and prone to human error. Furthermore, while standard Large Language Models (LLMs) are highly capable conversational agents, they cannot be trusted with critical safety queries out-of-the-box because they are prone to \"hallucinating\" technical facts and lack innate knowledge of localized policy documents.\n",
    "\n",
    "**Purpose and Domain Use Case**\n",
    "The purpose of this project is to develop an LLM-powered chatbot tailored specifically to the domain of workplace safety policies and manuals. The intended use case is to serve as an interactive safety assistant for safety officers, employers, and workers. Users can query the system in natural language (e.g., \"What are the required dimensions for a machine guard?\") and the chatbot will instantly retrieve and synthesize the exact procedural guidelines and compliance protocols from the official DOLE OSHS text.\n",
    "\n",
    "**Real-World Significance**\n",
    "Building a retrieval-grounded conversational system (utilizing a Retrieval-Augmented Generation or RAG pipeline) is critical for this application. By anchoring the LLM's responses exclusively to retrieved chunks of the official OSHS document, we eliminate hallucinations and guarantee that the information provided is factual, reliable, and citeable. In a real-world setting, this system accelerates regulatory compliance, democratizes access to dense safety protocols, and ultimately helps mitigate workplace hazards by ensuring accurate safety knowledge is instantly accessible.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2. Dataset Description\n",
    "\n",
    "**Knowledge Source and Collection**\n",
    "The primary knowledge source for this chatbot is the **Occupational Safety and Health Standards (OSHS) As Amended** handbook, issued by the Department of Labor and Employment (DOLE) of the Philippines. The document was acquired as a digital PDF (closed-corpus) and serves as the definitive legal and regulatory baseline for occupational safety in the country. \n",
    "\n",
    "**Dataset Structure**\n",
    "* **Format:** Single PDF document (`Osh-Handbook.pdf`)\n",
    "* **Domain:** Legal, Regulatory, and Occupational Health & Safety\n",
    "* **Contents:** The document is highly structured, consisting of hierarchical legal frameworks (Rules, Sections, Sub-sections) alongside dense technical matrices (e.g., Threshold Limit Values for airborne contaminants, medical supply requirements).\n",
    "\n",
    "**Preprocessing and Data Pipeline**\n",
    "To ensure the LLM accurately retrieves and contextualizes the legal statutes without hallucination, standard naive chunking was discarded in favor of a **Structure-Aware Processing Pipeline**:\n",
    "\n",
    "1.  **Document Cleaning:** * **Artifact Removal:** Page numbers, headers, and extraneous source tags (e.g., `--- PAGE X ---`) were stripped using Regular Expressions to reduce embedding noise.\n",
    "    * **Hyphenation Merging:** Words split across line breaks by hyphens (e.g., \"equip-ment\") were systematically rejoined to maintain semantic integrity during vector search.\n",
    "2.  **Handling Tables:** * Complex tables embedded within the PDF are extracted independently using `pdfplumber`. These tables are converted into Markdown format before embedding to preserve their row-column relationships, ensuring that specific numerical limits and chemical properties remain explicitly linked to their respective entities.\n",
    "3.  **Structure-Aware Chunking & Metadata Tagging:** * The text is strictly partitioned using **Rule Numbers** (e.g., \"Rule 1040\") as the primary delimiters. \n",
    "    * **Context Injection:** To prevent orphaned text chunks from losing their legal context, the specific Rule Number and Title are prepended as metadata to every sub-chunk generated from that section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all required libraries once (run this after Section 2)\n",
    "!pip install -U --force-reinstall \\\n",
    "    numpy==1.26.4 protobuf==4.25.3 \\\n",
    "    transformers==4.46.3 sentence-transformers==3.3.1 peft==0.12.0 \\\n",
    "    accelerate==0.34.2 bitsandbytes==0.49.2 \\\n",
    "    langchain==0.3.11 langchain-core==0.3.24 langchain-community==0.3.11 \\\n",
    "    langchain-huggingface==0.1.2 langchain-text-splitters==0.3.2 \\\n",
    "    langchain-chroma==0.1.4 \\\n",
    "    chromadb==0.5.23 pdfplumber==0.11.4 pandas==2.2.3 tabulate==0.9.0 gradio==5.9.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.1 Dataset Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment Setup and Imports\n",
    "Run this first to install the required libraries and import the modules. tabulate is required for pandas to convert tables to Markdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pdfplumber'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Import modules\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mre\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpdfplumber\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_text_splitters\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RecursiveCharacterTextSplitter\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pdfplumber'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pdfplumber\n",
      "  Downloading pdfplumber-0.11.9-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting langchain\n",
      "  Downloading langchain-1.2.10-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting langchain-text-splitters\n",
      "  Downloading langchain_text_splitters-1.1.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting pandas\n",
      "  Downloading pandas-3.0.1-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
      "Collecting tabulate\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Collecting pdfminer.six==20251230 (from pdfplumber)\n",
      "  Downloading pdfminer_six-20251230-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting Pillow>=9.1 (from pdfplumber)\n",
      "  Downloading pillow-12.1.1-cp312-cp312-win_amd64.whl.metadata (9.0 kB)\n",
      "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
      "  Downloading pypdfium2-5.5.0-py3-none-win_amd64.whl.metadata (68 kB)\n",
      "Collecting charset-normalizer>=2.0.0 (from pdfminer.six==20251230->pdfplumber)\n",
      "  Downloading charset_normalizer-3.4.4-cp312-cp312-win_amd64.whl.metadata (38 kB)\n",
      "Collecting cryptography>=36.0.0 (from pdfminer.six==20251230->pdfplumber)\n",
      "  Downloading cryptography-46.0.5-cp311-abi3-win_amd64.whl.metadata (5.7 kB)\n",
      "Collecting langchain-core<2.0.0,>=1.2.10 (from langchain)\n",
      "  Downloading langchain_core-1.2.14-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting langgraph<1.1.0,>=1.0.8 (from langchain)\n",
      "  Downloading langgraph-1.0.9-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting pydantic<3.0.0,>=2.7.4 (from langchain)\n",
      "  Downloading pydantic-2.12.5-py3-none-any.whl.metadata (90 kB)\n",
      "Collecting numpy>=1.26.0 (from pandas)\n",
      "  Downloading numpy-2.4.2-cp312-cp312-win_amd64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\kurt\\onedrive\\documents\\projects\\osh-standards-chatbot\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting tzdata (from pandas)\n",
      "  Downloading tzdata-2025.3-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting jsonpatch<2.0.0,>=1.33.0 (from langchain-core<2.0.0,>=1.2.10->langchain)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting langsmith<1.0.0,>=0.3.45 (from langchain-core<2.0.0,>=1.2.10->langchain)\n",
      "  Downloading langsmith-0.7.6-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: packaging>=23.2.0 in c:\\users\\kurt\\onedrive\\documents\\projects\\osh-standards-chatbot\\.venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.10->langchain) (26.0)\n",
      "Collecting pyyaml<7.0.0,>=5.3.0 (from langchain-core<2.0.0,>=1.2.10->langchain)\n",
      "  Downloading pyyaml-6.0.3-cp312-cp312-win_amd64.whl.metadata (2.4 kB)\n",
      "Collecting tenacity!=8.4.0,<10.0.0,>=8.1.0 (from langchain-core<2.0.0,>=1.2.10->langchain)\n",
      "  Downloading tenacity-9.1.4-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting typing-extensions<5.0.0,>=4.7.0 (from langchain-core<2.0.0,>=1.2.10->langchain)\n",
      "  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting uuid-utils<1.0,>=0.12.0 (from langchain-core<2.0.0,>=1.2.10->langchain)\n",
      "  Downloading uuid_utils-0.14.1-cp39-abi3-win_amd64.whl.metadata (4.9 kB)\n",
      "Collecting langgraph-checkpoint<5.0.0,>=2.1.0 (from langgraph<1.1.0,>=1.0.8->langchain)\n",
      "  Downloading langgraph_checkpoint-4.0.0-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting langgraph-prebuilt<1.1.0,>=1.0.8 (from langgraph<1.1.0,>=1.0.8->langchain)\n",
      "  Downloading langgraph_prebuilt-1.0.8-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting langgraph-sdk<0.4.0,>=0.3.0 (from langgraph<1.1.0,>=1.0.8->langchain)\n",
      "  Downloading langgraph_sdk-0.3.8-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting xxhash>=3.5.0 (from langgraph<1.1.0,>=1.0.8->langchain)\n",
      "  Downloading xxhash-3.6.0-cp312-cp312-win_amd64.whl.metadata (13 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.41.5 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Downloading pydantic_core-2.41.5-cp312-cp312-win_amd64.whl.metadata (7.4 kB)\n",
      "Collecting typing-inspection>=0.4.2 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Downloading typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\kurt\\onedrive\\documents\\projects\\osh-standards-chatbot\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Collecting cffi>=2.0.0 (from cryptography>=36.0.0->pdfminer.six==20251230->pdfplumber)\n",
      "  Downloading cffi-2.0.0-cp312-cp312-win_amd64.whl.metadata (2.6 kB)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.10->langchain)\n",
      "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting ormsgpack>=1.12.0 (from langgraph-checkpoint<5.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.8->langchain)\n",
      "  Downloading ormsgpack-1.12.2-cp312-cp312-win_amd64.whl.metadata (3.3 kB)\n",
      "Collecting httpx>=0.25.2 (from langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.8->langchain)\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting orjson>=3.11.5 (from langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.8->langchain)\n",
      "  Downloading orjson-3.11.7-cp312-cp312-win_amd64.whl.metadata (43 kB)\n",
      "Collecting requests-toolbelt>=1.0.0 (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.10->langchain)\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting requests>=2.0.0 (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.10->langchain)\n",
      "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting zstandard>=0.23.0 (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.10->langchain)\n",
      "  Downloading zstandard-0.25.0-cp312-cp312-win_amd64.whl.metadata (3.3 kB)\n",
      "Collecting pycparser (from cffi>=2.0.0->cryptography>=36.0.0->pdfminer.six==20251230->pdfplumber)\n",
      "  Downloading pycparser-3.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting anyio (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.8->langchain)\n",
      "  Downloading anyio-4.12.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting certifi (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.8->langchain)\n",
      "  Downloading certifi-2026.1.4-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting httpcore==1.* (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.8->langchain)\n",
      "  Downloading httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting idna (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.8->langchain)\n",
      "  Downloading idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.8->langchain)\n",
      "  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.10->langchain)\n",
      "  Downloading urllib3-2.6.3-py3-none-any.whl.metadata (6.9 kB)\n",
      "Downloading pdfplumber-0.11.9-py3-none-any.whl (60 kB)\n",
      "Downloading pdfminer_six-20251230-py3-none-any.whl (6.6 MB)\n",
      "   ---------------------------------------- 0.0/6.6 MB ? eta -:--:--\n",
      "   ----------- ---------------------------- 1.8/6.6 MB 10.1 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 3.1/6.6 MB 7.7 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 4.2/6.6 MB 7.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 5.8/6.6 MB 7.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.6/6.6 MB 7.0 MB/s eta 0:00:00\n",
      "Downloading langchain-1.2.10-py3-none-any.whl (111 kB)\n",
      "Downloading langchain_text_splitters-1.1.1-py3-none-any.whl (35 kB)\n",
      "Downloading pandas-3.0.1-cp312-cp312-win_amd64.whl (9.7 MB)\n",
      "   ---------------------------------------- 0.0/9.7 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 1.6/9.7 MB 7.6 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 3.7/9.7 MB 8.7 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 5.0/9.7 MB 7.9 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 6.8/9.7 MB 8.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 9.2/9.7 MB 8.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.7/9.7 MB 8.5 MB/s eta 0:00:00\n",
      "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Downloading langchain_core-1.2.14-py3-none-any.whl (501 kB)\n",
      "Downloading langgraph-1.0.9-py3-none-any.whl (158 kB)\n",
      "Downloading numpy-2.4.2-cp312-cp312-win_amd64.whl (12.3 MB)\n",
      "   ---------------------------------------- 0.0/12.3 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 2.9/12.3 MB 15.2 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 5.8/12.3 MB 13.6 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 8.7/12.3 MB 14.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.8/12.3 MB 14.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.3/12.3 MB 14.0 MB/s eta 0:00:00\n",
      "Downloading pillow-12.1.1-cp312-cp312-win_amd64.whl (7.0 MB)\n",
      "   ---------------------------------------- 0.0/7.0 MB ? eta -:--:--\n",
      "   -------------- ------------------------- 2.6/7.0 MB 12.6 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 5.5/7.0 MB 13.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.0/7.0 MB 12.8 MB/s eta 0:00:00\n",
      "Downloading pydantic-2.12.5-py3-none-any.whl (463 kB)\n",
      "Downloading pydantic_core-2.41.5-cp312-cp312-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.0/2.0 MB 14.1 MB/s eta 0:00:00\n",
      "Downloading pypdfium2-5.5.0-py3-none-win_amd64.whl (3.1 MB)\n",
      "   ---------------------------------------- 0.0/3.1 MB ? eta -:--:--\n",
      "   ------------------------------ --------- 2.4/3.1 MB 12.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.1/3.1 MB 11.4 MB/s eta 0:00:00\n",
      "Downloading tzdata-2025.3-py2.py3-none-any.whl (348 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading charset_normalizer-3.4.4-cp312-cp312-win_amd64.whl (107 kB)\n",
      "Downloading cryptography-46.0.5-cp311-abi3-win_amd64.whl (3.5 MB)\n",
      "   ---------------------------------------- 0.0/3.5 MB ? eta -:--:--\n",
      "   --------------------------- ------------ 2.4/3.5 MB 12.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.5/3.5 MB 12.1 MB/s eta 0:00:00\n",
      "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading langgraph_checkpoint-4.0.0-py3-none-any.whl (46 kB)\n",
      "Downloading langgraph_prebuilt-1.0.8-py3-none-any.whl (35 kB)\n",
      "Downloading langgraph_sdk-0.3.8-py3-none-any.whl (90 kB)\n",
      "Downloading langsmith-0.7.6-py3-none-any.whl (325 kB)\n",
      "Downloading pyyaml-6.0.3-cp312-cp312-win_amd64.whl (154 kB)\n",
      "Downloading tenacity-9.1.4-py3-none-any.whl (28 kB)\n",
      "Using cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Downloading typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Downloading uuid_utils-0.14.1-cp39-abi3-win_amd64.whl (187 kB)\n",
      "Downloading xxhash-3.6.0-cp312-cp312-win_amd64.whl (31 kB)\n",
      "Downloading cffi-2.0.0-cp312-cp312-win_amd64.whl (183 kB)\n",
      "Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Downloading httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading orjson-3.11.7-cp312-cp312-win_amd64.whl (125 kB)\n",
      "Downloading ormsgpack-1.12.2-cp312-cp312-win_amd64.whl (117 kB)\n",
      "Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Downloading zstandard-0.25.0-cp312-cp312-win_amd64.whl (506 kB)\n",
      "Downloading certifi-2026.1.4-py3-none-any.whl (152 kB)\n",
      "Downloading idna-3.11-py3-none-any.whl (71 kB)\n",
      "Downloading urllib3-2.6.3-py3-none-any.whl (131 kB)\n",
      "Downloading anyio-4.12.1-py3-none-any.whl (113 kB)\n",
      "Downloading pycparser-3.0-py3-none-any.whl (48 kB)\n",
      "Downloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Installing collected packages: zstandard, xxhash, uuid-utils, urllib3, tzdata, typing-extensions, tenacity, tabulate, pyyaml, pypdfium2, pycparser, Pillow, ormsgpack, orjson, numpy, jsonpointer, idna, h11, charset-normalizer, certifi, annotated-types, typing-inspection, requests, pydantic-core, pandas, jsonpatch, httpcore, cffi, anyio, requests-toolbelt, pydantic, httpx, cryptography, pdfminer.six, langsmith, langgraph-sdk, pdfplumber, langchain-core, langgraph-checkpoint, langchain-text-splitters, langgraph-prebuilt, langgraph, langchain\n",
      "Successfully installed Pillow-12.1.1 annotated-types-0.7.0 anyio-4.12.1 certifi-2026.1.4 cffi-2.0.0 charset-normalizer-3.4.4 cryptography-46.0.5 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 idna-3.11 jsonpatch-1.33 jsonpointer-3.0.0 langchain-1.2.10 langchain-core-1.2.14 langchain-text-splitters-1.1.1 langgraph-1.0.9 langgraph-checkpoint-4.0.0 langgraph-prebuilt-1.0.8 langgraph-sdk-0.3.8 langsmith-0.7.6 numpy-2.4.2 orjson-3.11.7 ormsgpack-1.12.2 pandas-3.0.1 pdfminer.six-20251230 pdfplumber-0.11.9 pycparser-3.0 pydantic-2.12.5 pydantic-core-2.41.5 pypdfium2-5.5.0 pyyaml-6.0.3 requests-2.32.5 requests-toolbelt-1.0.0 tabulate-0.9.0 tenacity-9.1.4 typing-extensions-4.15.0 typing-inspection-0.4.2 tzdata-2025.3 urllib3-2.6.3 uuid-utils-0.14.1 xxhash-3.6.0 zstandard-0.25.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries\n",
    "!pip install pdfplumber langchain langchain-text-splitters pandas tabulate\n",
    "\n",
    "# Import modules\n",
    "import re\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Define the file path (Ensure your PDF is uploaded to the Colab files section)\n",
    "PDF_PATH = \"Osh-Handbook.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Document Cleaning Utility\n",
    "This cell defines the function used to strip out page numbers, source tags, and fix broken words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Removes PDF artifacts and merges hyphenated words.\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove page artifacts like \"--- PAGE 1 ---\"\n",
    "    text = re.sub(r'--- PAGE \\d+ ---', '', text)\n",
    "    \n",
    "    # Merge hyphenated words across newlines (e.g., \"work-\\nplace\" -> \"workplace\")\n",
    "    text = re.sub(r'(\\w+)-\\n(\\w+)', r'\\1\\2', text)\n",
    "    \n",
    "    # Clean up excessive newlines\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
    "    \n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table Extraction\n",
    "This cell handles extracting complex tables and converting them into Markdown so the LLM can understand the rows and columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tables_to_documents(pdf_path):\n",
    "    print(\"Extracting tables...\")\n",
    "    table_documents = []\n",
    "    \n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page_num, page in enumerate(pdf.pages):\n",
    "            tables = page.extract_tables()\n",
    "            for i, table in enumerate(tables):\n",
    "                # Minimum content filter for tables\n",
    "                if not table or len(table) < 2: \n",
    "                    continue \n",
    "                \n",
    "                # Harden headers: Convert None to \"\" and ensure unique column names\n",
    "                raw_headers = [str(col) if col is not None else f\"Col_{j}\" for j, col in enumerate(table[0])]\n",
    "                \n",
    "                # Deduplicate headers if PDF parsing messed up (e.g., two columns named \"Limit\")\n",
    "                headers = pd.Series(raw_headers).mask(pd.Series(raw_headers).duplicated(), \n",
    "                                                      pd.Series(raw_headers) + '_dup').tolist()\n",
    "                \n",
    "                try:\n",
    "                    df = pd.DataFrame(table[1:], columns=headers).dropna(how='all')\n",
    "                    df = df.fillna(\"\") \n",
    "                    md_table = df.to_markdown(index=False)\n",
    "                    \n",
    "                    # Deduplication/Noise filter: Skip tiny or empty tables\n",
    "                    if len(md_table.strip()) < 50:\n",
    "                        continue\n",
    "                        \n",
    "                    # Create structured LangChain Document\n",
    "                    doc = Document(\n",
    "                        page_content=f\"[Table Extracted from Page {page_num + 1}]\\n{md_table}\",\n",
    "                        metadata={\n",
    "                            \"source\": \"Osh-Handbook.pdf\",\n",
    "                            \"page\": page_num + 1,\n",
    "                            \"type\": \"table\",\n",
    "                            \"table_index\": i\n",
    "                        }\n",
    "                    )\n",
    "                    table_documents.append(doc)\n",
    "                except Exception as e:\n",
    "                    print(f\"Skipped broken table on page {page_num + 1}: {e}\")\n",
    "                \n",
    "    print(f\"Successfully extracted {len(table_documents)} table documents.\")\n",
    "    return table_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Extraction & Structure-Aware Chunking\n",
    "This is the core logic. It reads the text, splits it by DOLE Rules, and prepends the Rule Title to every sub-chunk so context is never lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dole_rules_to_documents(pdf_path):\n",
    "    print(\"Extracting and cleaning text...\")\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=150,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    "    )\n",
    "    \n",
    "    text_documents = []\n",
    "    seen_chunks = set()\n",
    "    \n",
    "    # --- STATE PERSISTENCE VARIABLES ---\n",
    "    # Defined OUTSIDE the page loop so they survive page transitions\n",
    "    current_rule_id = \"General\"\n",
    "    current_rule_title = \"General OSHS Provision\"\n",
    "    \n",
    "    print(\"Chunking rules and assigning metadata...\")\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page_num, page in enumerate(pdf.pages, start=1):\n",
    "            extracted = page.extract_text()\n",
    "            if not extracted:\n",
    "                continue\n",
    "            \n",
    "            # Apply your cleaning function\n",
    "            cleaned_page_text = clean_text(extracted)\n",
    "            if not cleaned_page_text:\n",
    "                continue\n",
    "            \n",
    "            # Prepend a newline to ensure regex catches a Rule if it starts at the very top of the page\n",
    "            cleaned_page_text = \"\\n\" + cleaned_page_text\n",
    "            \n",
    "            # Split by Rule headers\n",
    "            # logic: (?i) case-insensitive, \\n matches newline before Rule\n",
    "            rule_splits = re.split(r'(?i)\\n(?=Rule\\s\\d{4})', cleaned_page_text)\n",
    "            \n",
    "            for section in rule_splits:\n",
    "                section = section.strip()\n",
    "                if len(section) < 50:\n",
    "                    continue\n",
    "                \n",
    "                # CHECK: Does this section START with a new Rule Header?\n",
    "                first_line = section.split('\\n')[0]\n",
    "                rule_match = re.match(r'(?i)Rule\\s(\\d{4})', first_line)\n",
    "                \n",
    "                if rule_match:\n",
    "                    # YES: We found a new rule. Update the \"State\".\n",
    "                    current_rule_id = rule_match.group(1)\n",
    "                    current_rule_title = first_line.strip()\n",
    "                else:\n",
    "                    # NO: This is continuation text from the previous page/rule.\n",
    "                    # We KEEP using the existing 'current_rule_id' and 'current_rule_title'\n",
    "                    pass \n",
    "                \n",
    "                # Now chunk using the correct context (whether new or inherited)\n",
    "                sub_chunks = text_splitter.split_text(section)\n",
    "                \n",
    "                for chunk in sub_chunks:\n",
    "                    normalized_chunk = chunk.strip()\n",
    "                    if len(normalized_chunk.split()) < 10:\n",
    "                        continue\n",
    "                    \n",
    "                    dedup_key = (current_rule_id, normalized_chunk)\n",
    "                    if dedup_key in seen_chunks:\n",
    "                        continue\n",
    "                    seen_chunks.add(dedup_key)\n",
    "                    \n",
    "                    doc = Document(\n",
    "                        page_content=f\"[{current_rule_title}]\\n{normalized_chunk}\",\n",
    "                        metadata={\n",
    "                            \"source\": \"Osh-Handbook.pdf\",\n",
    "                            \"rule_id\": current_rule_id, # Uses the persisted state\n",
    "                            \"rule_title\": current_rule_title,\n",
    "                            \"type\": \"text\",\n",
    "                            \"page\": page_num\n",
    "                        }\n",
    "                    )\n",
    "                    text_documents.append(doc)\n",
    "            \n",
    "    print(f\"Generated {len(text_documents)} structured text documents.\")\n",
    "    return text_documents\n",
    "\n",
    "# Execution\n",
    "tables_docs = extract_tables_to_documents(PDF_PATH)\n",
    "text_docs = process_dole_rules_to_documents(PDF_PATH)\n",
    "all_knowledge_base_docs = text_docs + tables_docs\n",
    "\n",
    "# Preview the rich metadata\n",
    "# --- FINAL PREVIEW BLOCK ---\n",
    "\n",
    "print(\"\\n--- Document Object Preview ---\")\n",
    "\n",
    "# 1. The \"Empty-List Guard\": Check if the list actually has items\n",
    "if not all_knowledge_base_docs:\n",
    "    print(\"Warning: No documents were generated. Please check your PDF path and extraction logic.\")\n",
    "else:\n",
    "    # 2. The Dynamic Index: Use index 50, OR the very last index if the list is smaller than 51\n",
    "    preview_index = min(50, len(all_knowledge_base_docs) - 1)\n",
    "    \n",
    "    print(f\"Previewing Document at Index {preview_index}:\")\n",
    "    print(f\"Content: {all_knowledge_base_docs[preview_index].page_content[:150]}...\")\n",
    "    print(f\"Metadata: {all_knowledge_base_docs[preview_index].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine & Final Check\n",
    "Run this cell to combine your extracted tables and text chunks into one unified knowledge base list, ready to be embedded and stored in ChromaDB in your next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine text and table documents\n",
    "all_knowledge_base_docs = text_docs + tables_docs\n",
    "\n",
    "print(f\"Total Text Docs: {len(text_docs)}\")\n",
    "print(f\"Total Table Docs: {len(tables_docs)}\")\n",
    "print(f\"Total Combined Docs ready for Vector DB: {len(all_knowledge_base_docs)}\")\n",
    "\n",
    "# This list 'all_knowledge_base_docs' is what you will pass to your embedding model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.2 Comparing Embedding Models\n",
    "\n",
    "Before finalizing the system architecture, a evaluation of different embedding models is necessary to determine which performs best on the DOLE OSHS legal text. The goal is to find a model that balances technical language comprehension, semantic accuracy, and cross-lingual (Taglish) capabilities.\n",
    "\n",
    "The following 4 models are evaluated in this Embedding Evaluation:\n",
    "1. **`all-MiniLM-L6-v2`**: The industry standard for lightweight, fast semantic search. Serves as our baseline.\n",
    "2. **`all-mpnet-base-v2`**: A heavier, highly accurate pure-English model from Sentence Transformers.\n",
    "3. **`BAAI/bge-small-en-v1.5`**: A state-of-the-art open-source model on the MTEB leaderboard, known for handling dense technical retrieval.\n",
    "4. **`paraphrase-multilingual-MiniLM-L12-v2`**: A multilingual model tested specifically for its ability to map Tagalog/Taglish queries to English regulatory text.\n",
    "\n",
    "**Methodology:**\n",
    "The cleaned, structure-aware `Document` objects are embedded into temporary ChromaDB vector stores. A mini test-suite of 5 diverse queries (covering English technical, Taglish, and Table lookups) is passed to each model. We evaluate them **qualitatively** (by reviewing the retrieved context) and **quantitatively** by comparing the L2 Distance scores (where a lower score indicates higher mathematical similarity between the query and the retrieved document)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup and Model Definition\n",
    "Run this cell to define the 4 models you are going to test. This uses the updated, highly curated list we discussed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries for Vector Store and Embeddings\n",
    "!pip install chromadb sentence-transformers langchain-huggingface langchain-community pandas tabulate\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Define the 4 models for the ablation study\n",
    "models_to_test = {\n",
    "    \"MiniLM (Baseline)\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    \"MPNet (Heavy English)\": \"sentence-transformers/all-mpnet-base-v2\",\n",
    "    \"BGE-Small (Technical)\": \"BAAI/bge-small-en-v1.5\",\n",
    "    \"Multilingual (Taglish)\": \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "}\n",
    "\n",
    "vector_stores_arena = {}\n",
    "\n",
    "print(\"Models loaded into the Arena configuration:\")\n",
    "for name in models_to_test.keys():\n",
    "    print(f\"- {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the Vector Databases (The Heavy Lifting)\n",
    "This cell will download each model and embed your documents. Note: Depending on your Colab GPU, this might take a few minutes to complete since it is processing 4 different models back-to-back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Starting Embedding Model Evaluation ===\")\n",
    "\n",
    "# Ensure we actually have documents to test\n",
    "if 'all_knowledge_base_docs' not in locals() or not all_knowledge_base_docs:\n",
    "    print(\"Error: all_knowledge_base_docs is empty or not defined. Please run Section 2.1 first.\")\n",
    "else:\n",
    "    # 2. Build an in-memory ChromaDB for each model\n",
    "    for model_nickname, model_path in models_to_test.items():\n",
    "        print(f\"\\nInitializing {model_nickname}...\")\n",
    "        \n",
    "        # Load the embedding model (utilizing Colab T4 GPU)\n",
    "        embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=model_path,\n",
    "            model_kwargs={'device': 'cuda'}, \n",
    "            encode_kwargs={'normalize_embeddings': True} \n",
    "        )\n",
    "        \n",
    "        # Create an in-memory vector store (no persist_directory)\n",
    "        print(f\"Embedding chunks into temporary Vector Store...\")\n",
    "        vectorstore = Chroma.from_documents(\n",
    "            documents=all_knowledge_base_docs,\n",
    "            embedding=embeddings\n",
    "        )\n",
    "        \n",
    "        vector_stores_arena[model_nickname] = vectorstore\n",
    "\n",
    "    print(\"\\n=== All Temporary Databases Built Successfully! ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quantitative & Qualitative Evaluation\n",
    "This is where the magic happens. We use similarity_search_with_score to extract the mathematical distance, and we wrap it in a Pandas DataFrame to output a beautiful comparison table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a mini test-suite representing different user intents\n",
    "test_queries = [\n",
    "    \"What are the requirements for a Safety Committee in a high-risk workplace?\", # Standard English\n",
    "    \"What is the threshold limit value for Lead and Arsenic?\", # Table/Chemical lookup\n",
    "    \"Who is responsible for providing personal protective equipment?\", # Policy/Responsibility\n",
    "    \"Ilang safety officer ang kailangan sa construction site na may 300 workers?\", # Taglish/Filipino\n",
    "    \"Ano ang parusa sa hindi pagsunod sa OSH standards?\" # Taglish/Penalties\n",
    "]\n",
    "\n",
    "# We will store the L2 Distance scores (lower is better) to quantitatively compare models\n",
    "results_data = []\n",
    "\n",
    "print(\"=== Quantitative & Qualitative Embedding Evaluation ===\")\n",
    "\n",
    "if not vector_stores_arena:\n",
    "    print(\"Error: Vector stores not built. Run the previous cells.\")\n",
    "else:\n",
    "    for query in test_queries:\n",
    "        print(f\"\\n\\n--- TEST QUERY: '{query}' ---\")\n",
    "        \n",
    "        # Initialize a dictionary for our DataFrame row\n",
    "        query_row = {\"Query\": query[:35] + \"...\"}\n",
    "        \n",
    "        for model_nickname, vectorstore in vector_stores_arena.items():\n",
    "            # Get top 1 result and its distance score (ChromaDB defaults to L2 distance)\n",
    "            results = vectorstore.similarity_search_with_score(query, k=1)\n",
    "            \n",
    "            if results:\n",
    "                top_doc, score = results[0]\n",
    "                \n",
    "                # Print Qualitative Result\n",
    "                print(f\"\\n>>> [{model_nickname}] (L2 Distance: {score:.4f})\")\n",
    "                print(f\"Rule: {top_doc.metadata.get('rule_id')} - {top_doc.metadata.get('rule_title')} (Page {top_doc.metadata.get('page')})\")\n",
    "                print(f\"Preview: {top_doc.page_content.replace('\\n', ' ')[:100]}...\")\n",
    "                \n",
    "                # Save Quantitative Result (Score) for the table\n",
    "                query_row[model_nickname] = round(score, 4)\n",
    "        \n",
    "        results_data.append(query_row)\n",
    "    \n",
    "    # --- DISPLAY QUANTITATIVE SUMMARY TABLE ---\n",
    "    print(\"\\n\\n\" + \"=\"*70)\n",
    "    print(\"=== QUANTITATIVE SUMMARY (L2 Distance - Lower is Better) ===\")\n",
    "    print(\"=\"*70)\n",
    "    df_results = pd.DataFrame(results_data)\n",
    "    \n",
    "    # Print as a clean Markdown table\n",
    "    print(df_results.to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation Metric\n",
    "The quantitative results indicated that `BAAI/bge-small-en-v1.5` is the superior embedding model for the DOLE OSHS dataset. It achieved the lowest L2 Distance scores across all five test categories, including a significant margin of victory in dense table lookups (0.5236). Notably, it also outperformed the dedicated multilingual model on Taglish queries, likely due to its superior handling of the English technical loan words embedded within the Filipino syntax. \n",
    "\n",
    "Therefore, `bge-small-en-v1.5` is selected as the permanent embedding model for the final RAG pipeline in Section 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3. Requirements\n",
    "\n",
    "To construct a reliable, hallucination-free Retrieval-Augmented Generation (RAG) pipeline for the DOLE OSHS handbook, the following frameworks and libraries were selected based on performance, open-source availability, and hardware constraints (Google Colab Pro T4 GPU):\n",
    "\n",
    "**1. Large Language Model (LLM):**\n",
    "* **`Llama 3.1 8B Instruct`**: Selected as the primary reasoning engine. It is highly optimized for instruction-following and \"closed-corpus\" tasks, ensuring that the model adheres strictly to the provided OSHS context and minimizes the risk of hallucination.\n",
    "\n",
    "**2. Embedding Model:**\n",
    "* **`BAAI/bge-small-en-v1.5`**: Chosen following a rigorous ablation study. It demonstrated the highest mathematical accuracy in retrieving dense technical English and PDF table contents, outperforming standard baseline models.\n",
    "\n",
    "**3. Vector Database:**\n",
    "* **`ChromaDB`**: An open-source, locally hosted vector database. It allows for efficient storage and similarity searching of high-dimensional vectors with integrated metadata filtering.\n",
    "\n",
    "**4. Backend and UI Tool:**\n",
    "* **`Gradio`**: Selected for the final web deployment. Gradio offers native \"notebook-first\" support for Google Colab, allowing for a stable, interactive chat interface without the need for complex external tunneling.\n",
    "\n",
    "**5. Additional Utilities:**\n",
    "* **`pdfplumber` & `pandas`**: Used for high-fidelity extraction of structured rules and tabular matrices from the OSHS PDF.\n",
    "* **`LangChain`**: The orchestration framework used to link the retriever, the prompt template, and the LLM into a unified RAG chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SECTION 3: MASTER REQUIREMENTS & SETUP ---\n",
    "\n",
    "# 1. Install final required libraries\n",
    "# Packages are already installed in the setup cell below Section 2.\n",
    "\n",
    "# 2. Import core components\n",
    "import gradio as gr\n",
    "# --- UPDATED IMPORTS FOR LANGCHAIN v0.3+ ---\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Keep these as they are (these are in the community and huggingface extensions)\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "print(\"All OSHS Chatbot requirements successfully installed and imported.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4. System Architecture\n",
    "\n",
    "The chatbot uses a Retrieval-Augmented Generation (RAG) architecture to keep outputs grounded in the DOLE OSHS corpus.\n",
    "\n",
    "**Overall Architecture**\n",
    "- **Retriever:** LangChain retriever configured on the final vector store (`k=3`) to fetch top relevant chunks.\n",
    "- **Vector Store:** Persistent `Chroma` collection stored in Google Drive for reuse across sessions.\n",
    "- **Embedding Model:** `BAAI/bge-small-en-v1.5` used for document and query embeddings.\n",
    "- **LLM:** `Llama 3.1 8B Instruct` used for final answer generation.\n",
    "- **Prompt Template:** A strict template that injects retrieved context and constrains answers to source-grounded information.\n",
    "\n",
    "**Pipeline (query → embedding → similarity search → context injection)**\n",
    "1. User submits a safety/compliance query.\n",
    "2. Query is embedded with `BAAI/bge-small-en-v1.5`.\n",
    "3. Similarity search in `Chroma` retrieves top-k relevant chunks.\n",
    "4. Retrieved chunks + metadata are injected into the prompt template.\n",
    "5. LLM generates an answer using grounded context.\n",
    "\n",
    "**Prompt Design and Grounding Strategy**\n",
    "- The prompt explicitly instructs the model to answer from retrieved context only.\n",
    "- Retrieved chunks preserve rule/page metadata to improve traceability.\n",
    "- Low temperature (`0.1`) reduces generative variance and hallucination risk.\n",
    "- If evidence is insufficient, the response should avoid unsupported claims."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### System Flow Diagram and Pseudocode\n",
    "\n",
    "```text\n",
    "function answer_query(user_query):\n",
    "    q_vec = embed(user_query, model='BAAI/bge-small-en-v1.5')\n",
    "    docs = chroma.similarity_search(q_vec, k=3)\n",
    "    prompt = build_prompt(user_query, docs)\n",
    "    answer = llama_generate(prompt, temperature=0.1)\n",
    "    return answer\n",
    "```\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    A[User Query] --> B[Embed Query\\nBGE-Small]\n",
    "    B --> C[Chroma Similarity Search\\nTop-k Chunks]\n",
    "    C --> D[Context Injection\\nPrompt Template]\n",
    "    D --> E[Llama 3.1 8B Instruct]\n",
    "    E --> F[Grounded Answer]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lock in the Retriever\n",
    "This cell builds your final, permanent database using the winning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from google.colab import drive\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# --- FIX 1 & 2: Google Drive Persistence and Device Agnostic Fallback ---\n",
    "print(\"Mounting Google Drive for persistent database storage...\")\n",
    "drive.mount('/content/drive')\n",
    "persist_directory = \"/content/drive/MyDrive/OSHS_ChromaDB_Final\"\n",
    "\n",
    "# Dynamically set device to avoid crashing if GPU is detached\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Initializing final embedding model (BGE-Small) on {device}...\")\n",
    "\n",
    "final_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\",\n",
    "    model_kwargs={'device': device}, \n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "# --- FIX 3: Prevent Duplication by checking if DB exists ---\n",
    "if os.path.exists(persist_directory) and os.listdir(persist_directory):\n",
    "    print(\"Loading existing Vector Database from Google Drive (preventing duplication)...\")\n",
    "    final_vectorstore = Chroma(\n",
    "        persist_directory=persist_directory, \n",
    "        embedding_function=final_embeddings\n",
    "    )\n",
    "else:\n",
    "    print(\"Building NEW Vector Database and saving to Google Drive...\")\n",
    "    final_vectorstore = Chroma.from_documents(\n",
    "        documents=all_knowledge_base_docs,\n",
    "        embedding=final_embeddings,\n",
    "        persist_directory=persist_directory\n",
    "    )\n",
    "\n",
    "# Convert the database into a LangChain \"Retriever\"\n",
    "retriever = final_vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "print(\"Retriever successfully built and ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Llama 3.1 8B (The LLM)\n",
    "This is the most crucial step. Because Llama 3.1 is an advanced model, it requires 4-bit quantization to fit on your Colab Pro GPU.\n",
    "\n",
    "CRITICAL PREREQUISITE: Llama 3.1 is a \"gated\" model. To download it, you must have a free Hugging Face account, accept Meta's terms on the Llama 3.1 page, and create an Access Token. You need to put your token in the Colab \"Secrets\" tab (the little key icon on the left sidebar) and name it HF_TOKEN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "import requests\n",
    "\n",
    "try:\n",
    "    # 1. Check if the secret exists in Colab\n",
    "    token = userdata.get('HF_TOKEN')\n",
    "    print(\"✅ Success: 'HF_TOKEN' found in Colab Secrets.\")\n",
    "    \n",
    "    # 2. Check if the token is valid by pinging Hugging Face API\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "    response = requests.get(\"https://huggingface.co/api/whoami-v2\", headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        user_info = response.json()\n",
    "        print(f\"✅ Success: Token is valid. Authenticated as: {user_info.get('name')}\")\n",
    "    else:\n",
    "        print(f\"❌ Error: Token invalid or expired (Status Code: {response.status_code})\")\n",
    "        print(\"Response:\", response.text)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: Could not find 'HF_TOKEN'. Ensure the toggle is 'ON' in the Secrets tab.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from google.colab import userdata\n",
    "\n",
    "# --- LOAD THE LLM (LLAMA 3.1 8B) ---\n",
    "print(\"Downloading and quantizing Llama 3.1 8B (This may take 2-3 minutes)...\")\n",
    "\n",
    "# Retrieve token\n",
    "hf_token = userdata.get('HF_TOKEN')\n",
    "model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "# Configure 4-bit Quantization (Shrinks the model to fit on the T4 GPU)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load Tokenizer and Model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token=hf_token)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    token=hf_token\n",
    ")\n",
    "\n",
    "# Create the HuggingFace Pipeline\n",
    "text_generation_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,      # Limit answer length to prevent rambling\n",
    "    temperature=0.1,         # Keep temperature low for factual, legal answers\n",
    "    repetition_penalty=1.1,\n",
    "    return_full_text=False   # Only return the generated answer, not the prompt\n",
    ")\n",
    "\n",
    "# Wrap it in LangChain so it can be used in the LCEL chain\n",
    "llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
    "\n",
    "print(\"Llama 3.1 8B Instruct loaded successfully and ready for RAG!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
